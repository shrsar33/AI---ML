{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Popular Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Identify handwriting on checks and classify each letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the ConvNet model\n",
    "class HandwrittenLetterClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandwrittenLetterClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 128)\n",
    "        self.fc2 = nn.Linear(128, 26)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 6 * 6)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load the dataset (assuming we have a dataset of handwritten letters)\n",
    "# For demonstration purposes, let's assume we have a dataset of handwritten letters with 26 classes (A-Z)\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "# dataset = torchvision.datasets.ImageFolder(root='path_to_dataset', transform=transform)\n",
    "\n",
    "# Define the data loaders\n",
    "# train_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = HandwrittenLetterClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "# for epoch in range(5):\n",
    "#     for images, labels in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# Use the model to classify handwritten letters on checks\n",
    "# check_image = ... (load and preprocess the check image)\n",
    "# output = model(check_image)\n",
    "# _, predicted = torch.max(output, 1)\n",
    "# print(\"Predicted letter:\", chr(predicted.item() + 65))  # Assuming A=0, B=1, ..., Z=25\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Task 2: Detect gender of a speaker based on voice data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import os\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = '/path/to/dataset'\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "male_files = ['male_file1.wav', 'male_file2.wav', 'male_file3.wav']\n",
    "female_files = ['female_file1.wav', 'female_file2.wav', 'female_file3.wav']\n",
    "\n",
    "# Function to load voice data\n",
    "def load_voice_data(file_path):\n",
    "    signal, sr = librosa.load(file_path)\n",
    "    mfccs = librosa.feature.mfcc(signal, sr=sr)\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)\n",
    "    return mfccs_mean\n",
    "\n",
    "# Load the dataset\n",
    "X = []\n",
    "y = []\n",
    "for file in male_files:\n",
    "    file_path = os.path.join(dataset_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        X.append(load_voice_data(file_path))\n",
    "        y.append(0)  # 0 for male\n",
    "    else:\n",
    "        print(f\"File {file} not found.\")\n",
    "for file in female_files:\n",
    "    file_path = os.path.join(dataset_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        X.append(load_voice_data(file_path))\n",
    "        y.append(1)  # 1 for female\n",
    "    else:\n",
    "        print(f\"File {file} not found.\")\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Check if X and y are not empty\n",
    "if len(X) > 0 and len(y) > 0:\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create a Random Forest classifier\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Train the classifier\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "else:\n",
    "    print(\"No files found in the dataset.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Classify email topics based on content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.DataFrame({\n",
    "    'content': ['This is a meeting invitation.', 'Your payment is due.', 'We have a new product launch.', 'Your account has been compromised.'],\n",
    "    'topic': ['Meeting', 'Payment', 'Product', 'Security']\n",
    "})\n",
    "\n",
    "# Function to preprocess the content\n",
    "def preprocess_content(content):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(content.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the content\n",
    "data['content'] = data['content'].apply(preprocess_content)\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = data['content']\n",
    "y = data['topic']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the training data and transform both the training and testing data\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test_vectorized)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
